{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mppenfold/AIHC5010-Winter-2026/blob/main/Assignment1_Colab_Workflow_JZ.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# Assignment 1 — Colab Workflow (GitHub + Pre-commit + Submission Validation)\n",
    "\n",
    "This notebook teaches the standard workflow used throughout the course:\n",
    "\n",
    "1. Clone your team repo\n",
    "2. Install dependencies\n",
    "3. Install **pre-commit** and enable a hook to strip notebook outputs\n",
    "4. Run this notebook end-to-end\n",
    "5. Validate `predictions.csv`\n",
    "6. Commit + push + tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1",
    "outputId": "8870b529-ab7a-424e-d055-63f49b4abad8"
   },
   "outputs": [],
   "source": [
    "# (Colab) show python and system info\n",
    "import sys, platform\n",
    "print(sys.version)\n",
    "print(platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "id": "2"
   },
   "source": [
    "## 1) Clone Repo\n",
    "\n",
    "Login to your personal Github account, and make a fork of: https://github.com/TLKline/AIHC-5010-Winter-2026\n",
    "\n",
    "Follow setup directions for working with a PAT in GitHub (30-second guide):\n",
    "\n",
    "* Go to GitHub → Settings\n",
    "* Developer settings\n",
    "* Personal access tokens\n",
    "* Choose:\n",
    "  * Fine-Grained\n",
    "\n",
    "You can clone using HTTPS.\n",
    "\n",
    "Repo HTTPS URL (e.g., `https://github.com/TLKline/AIHC-5010-Winter-2026.git`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3",
    "outputId": "537a2421-5ba7-4735-ac34-efe7adf3a745"
   },
   "outputs": [],
   "source": [
    "# TODO: Change the following to your github repo path\n",
    "repo_path = 'https://github.com/joezein71/AIHC-5010-Winter-2026.git'\n",
    "!git clone {repo_path} student_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4",
    "outputId": "d5697aef-faf9-4d86-ea31-dfe26d8bf768"
   },
   "outputs": [],
   "source": [
    "# Move into repo\n",
    "%cd student_repo\n",
    "\n",
    "# Repo git info\n",
    "!git status\n",
    "\n",
    "# Where are we?\n",
    "print('----------')\n",
    "print('We are at:')\n",
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "5"
   },
   "source": [
    "## 2) Install dependencies\n",
    "\n",
    "This installs whatever is in `requirements.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "id": "6"
   },
   "outputs": [],
   "source": [
    "!pip -q install -r Project-1/readmit30/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "7"
   },
   "source": [
    "## 3) Enable pre-commit hook to strip notebook outputs\n",
    "\n",
    "This prevents giant notebooks and reduces merge/diff pain.\n",
    "\n",
    "One-time per clone:\n",
    "- `pre-commit install`\n",
    "\n",
    "After that, every `git commit` will strip outputs from `*.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8",
    "outputId": "e82b2ae5-7400-4ef4-b0a5-24327b068ce8"
   },
   "outputs": [],
   "source": [
    "!pip -q install pre-commit\n",
    "!pre-commit install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "9"
   },
   "source": [
    "#MAINSTART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "10"
   },
   "source": [
    "# 4) Submission Notebook (Template)\n",
    "\n",
    "Replace the baseline model with your team’s approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11",
    "outputId": "3888e397-15a4-4255-96af-cbd3de512a40"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "TRAIN_PATH = os.environ.get(\"TRAIN_PATH\", \"Project-1/readmit30/scripts/data/public/train.csv\")\n",
    "DEV_PATH   = os.environ.get(\"DEV_PATH\",   \"Project-1/readmit30/scripts/data/public/dev.csv\")\n",
    "TEST_PATH  = os.environ.get(\"TEST_PATH\",  \"Project-1/readmit30/scripts/data/public/public_test.csv\")\n",
    "OUT_PATH   = os.environ.get(\"OUT_PATH\",   \"predictions.csv\")\n",
    "\n",
    "print(\"TRAIN_PATH:\", TRAIN_PATH)\n",
    "print(\"DEV_PATH:\", DEV_PATH)\n",
    "print(\"TEST_PATH:\", TEST_PATH)\n",
    "print(\"OUT_PATH:\", OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "12"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "assert \"row_id\" in train.columns and \"readmit30\" in train.columns\n",
    "assert \"row_id\" in test.columns\n",
    "\n",
    "X_train = train.drop(columns=[\"readmit30\"])\n",
    "y_train = train[\"readmit30\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {
    "id": "603e5c89"
   },
   "source": [
    "### Summary of Training Dataset (Numerical Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "efe37206",
    "outputId": "672e352b-d787-4c79-b25d-bbcdd90dfd2e"
   },
   "outputs": [],
   "source": [
    "display(train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {
    "id": "25e928a7"
   },
   "source": [
    "### Summary of Training Dataset (Categorical Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "94637506",
    "outputId": "3ce18dc1-76ee-42fe-9d67-2f08e9efaca9"
   },
   "outputs": [],
   "source": [
    "for column in train.select_dtypes(include='object').columns:\n",
    "    print(f\"\\nValue counts for column: {column}\")\n",
    "    display(train[column].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {
    "id": "QYLjjH77f-q6"
   },
   "source": [
    "**Missingness in Training and testing datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "xU9kBEZdgDYQ",
    "outputId": "c22afee9-e008-4272-d7b4-c34fc3ec9398"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate missing values for training set\n",
    "missing_train_count = train.isnull().sum()\n",
    "missing_train_percent = (train.isnull().sum() / len(train) * 100)\n",
    "missing_train_df = pd.DataFrame({\n",
    "    'Train Missing Count': missing_train_count,\n",
    "    'Train Missing Percentage (%)': missing_train_percent\n",
    "})\n",
    "\n",
    "# Calculate missing values for test set\n",
    "missing_test_count = test.isnull().sum()\n",
    "missing_test_percent = (test.isnull().sum() / len(test) * 100)\n",
    "missing_test_df = pd.DataFrame({\n",
    "    'Test Missing Count': missing_test_count,\n",
    "    'Test Missing Percentage (%)': missing_test_percent\n",
    "})\n",
    "\n",
    "# Merge the two dataframes side-by-side\n",
    "combined_missing_df = pd.merge(\n",
    "    missing_train_df,\n",
    "    missing_test_df,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='outer' # Use outer join to include variables missing in only one set\n",
    ").fillna(0) # Fill NaN values (for variables missing in only one set) with 0\n",
    "\n",
    "# Display the combined table\n",
    "print(\"Combined Missing Data Analysis (Training vs. Test Set - All Variables):\")\n",
    "display(combined_missing_df.sort_values(by='Train Missing Count', ascending=False).head(10)) # Sort for better readability, but include all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "id": "d7f0b6f6"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Calculate missing values for training set\n",
    "missing_train_count = train.isnull().sum()\n",
    "missing_train_percent = (train.isnull().sum() / len(train) * 100)\n",
    "missing_train_df = pd.DataFrame({\n",
    "    'Train Missing Count': missing_train_count,\n",
    "    'Train Missing Percentage (%)': missing_train_percent\n",
    "})\n",
    "\n",
    "# Calculate missing values for test set\n",
    "missing_test_count = test.isnull().sum()\n",
    "missing_test_percent = (test.isnull().sum() / len(test) * 100)\n",
    "missing_test_df = pd.DataFrame({\n",
    "    'Test Missing Count': missing_test_count,\n",
    "    'Test Missing Percentage (%)': missing_test_percent\n",
    "})\n",
    "\n",
    "# Merge the two dataframes side-by-side\n",
    "combined_missing_df = pd.merge(\n",
    "    missing_train_df,\n",
    "    missing_test_df,\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    "    how='outer' # Use outer join to include variables missing in only one set\n",
    ").fillna(0) # Fill NaN values (for variables missing in only one set) with 0\n",
    "\n",
    "# Display the combined table\n",
    "print(\"Combined Missing Data Analysis (Training vs. Test Set - All Variables):\")\n",
    "display(combined_missing_df.sort_values(by='Train Missing Count', ascending=False).head(10)) # Sort for better readability, but include all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {
    "id": "15cdf1ec"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dev = pd.read_csv(DEV_PATH) # Load the development data\n",
    "\n",
    "print(\"Missing values in 'readmit30' column (training data):\", train['readmit30'].isnull().sum())\n",
    "print(\"Missing values in 'readmit30' column (development data):\", dev['readmit30'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "ad9f7a66"
   },
   "source": [
    "### Summary of Variables with Outliers\n",
    "\n",
    "Based on the box plots, the following numerical covariates show significant outliers:\n",
    "\n",
    "*   **`time_in_hospital`**: Exhibited data points far above the upper whisker, indicating unusually long hospital stays.\n",
    "*   **`num_lab_procedures`**: Showed several points beyond the whiskers, suggesting exceptionally high numbers of lab procedures for some patients.\n",
    "*   **`num_medications`**: Displayed a clear right-skewed distribution with many data points above the upper whisker, implying a considerable number of medications for some patients.\n",
    "*   **`number_outpatient`**: This column has a highly skewed distribution with numerous outliers, indicating a small fraction of patients have a very high number of outpatient visits.\n",
    "*   **`number_emergency`**: This also shows a strong presence of outliers, with some patients having a significantly higher number of emergency visits.\n",
    "*   **`number_inpatient`**: Similar to outpatient and emergency visits, this feature has many outliers, suggesting a subset of patients have had a much higher frequency of inpatient admissions.\n",
    "*   **`number_diagnoses`**: Contained some outliers at the higher end, representing patients with an unusually large number of diagnoses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "id": "6iqSCqKHSmzU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {
    "id": "w_B2t5aedEId"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {
    "id": "299d6ddd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'numerical_cols' is already defined from previous steps\n",
    "# If not, you'd redefine it here:\n",
    "# numerical_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "# if 'readmit30' in numerical_cols: numerical_cols.remove('readmit30')\n",
    "# if 'row_id' in numerical_cols: numerical_cols.remove('row_id')\n",
    "\n",
    "outlier_counts = {}\n",
    "\n",
    "for col in numerical_cols:\n",
    "    Q1 = train[col].quantile(0.25)\n",
    "    Q3 = train[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Count outliers\n",
    "    outliers = train[(train[col] < lower_bound) | (train[col] > upper_bound)]\n",
    "    outlier_counts[col] = len(outliers)\n",
    "\n",
    "print(\"Number of outliers per numerical variable (using IQR method):\")\n",
    "for col, count in outlier_counts.items():\n",
    "    print(f\"{col}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {
    "id": "d83cbf62"
   },
   "source": [
    "### Number of Duplicates per Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {
    "id": "b798c957"
   },
   "outputs": [],
   "source": [
    "duplicate_counts = {}\n",
    "\n",
    "for col in train.columns:\n",
    "    # Count duplicates (excluding the first occurrence)\n",
    "    duplicate_counts[col] = train[col].duplicated().sum()\n",
    "\n",
    "print(\"Number of duplicates per variable (excluding first occurrence):\")\n",
    "for col, count in duplicate_counts.items():\n",
    "    print(f\"{col}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {
    "id": "QdiDMcaZcU2H"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {
    "id": "c0193a94"
   },
   "source": [
    "# Task\n",
    "Analyze potential data leakage in the `train` dataset by visualizing the relationship between each feature and the target variable `readmit30`. Specifically:\n",
    "\n",
    "1.  For numerical features (excluding `encounter_id`, `patient_nbr`, `row_id`), create box plots or violin plots comparing their distributions for patients readmitted (`readmit30=1`) versus not readmitted (`readmit30=0`).\n",
    "2.  For categorical features (excluding `encounter_id`, `patient_nbr`, `row_id`), create stacked bar charts or count plots to show the proportion of `readmit30` outcomes within each category.\n",
    "\n",
    "Finally, summarize any features that exhibit an unusually strong or direct relationship with `readmit30`, indicating potential data leakage or highly predictive features that warrant further investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {
    "id": "1bdb71a1"
   },
   "source": [
    "### Summary of Features with Potential Data Leakage or High Predictiveness\n",
    "\n",
    "Based on the visualizations of numerical and categorical features against the `readmit30` target variable, several features show a strong relationship, which could indicate either high predictiveness or potential data leakage:\n",
    "\n",
    "**Numerical Features (from Box Plots):**\n",
    "\n",
    "*   **`discharge_disposition_id`**: Certain `discharge_disposition_id` values (e.g., those indicating death or transfer to another facility) show a clear separation in `readmit30` distributions. For instance, if a patient is discharged to a hospice or dies, they cannot be readmitted, leading to `readmit30=0`. This is a strong indicator of **data leakage** because the discharge disposition often occurs at the end of the hospital stay, and certain outcomes directly preclude readmission. Specifically, `discharge_disposition_id` values like '11' (Expired), '13' (Discharged/transferred to home under care of Home IV provider), '14' (Discharged/transferred to a non-acute care facility), '19' (Expired at home. Medicaid only, hospice), '20' (Expired in a medical facility. Medicaid only, hospice), '21' (Discharged/transferred to another institution for inpatient care), '24' (Expired at home - hospice), '25' (Expired in a medical facility - hospice) are highly indicative of no readmission.\n",
    "*   **`number_outpatient`, `number_emergency`, `number_inpatient`**: These features, particularly `number_outpatient` and `number_emergency`, show a trend where higher counts are associated with a higher likelihood of readmission. While not as direct as `discharge_disposition_id`, a very high number of prior outpatient, emergency, or inpatient visits for the same patient (`patient_nbr`) could be highly predictive, and in some contexts, could hint at `data leakage` if this information is recorded or becomes known *after* the decision for the current admission but before the 30-day readmission window.\n",
    "*   **`time_in_hospital`**: Patients with longer hospital stays tend to have a slightly higher median for `readmit30=1`, though the overlap is significant. This could be a genuine predictor rather than leakage.\n",
    "\n",
    "**Categorical Features (from Stacked Bar Charts):**\n",
    "\n",
    "*   **`discharge_disposition_id`**: Similar to its numerical interpretation, several categories within `discharge_disposition_id` directly correlate with `readmit30=0` (no readmission). For example, discharge dispositions indicating death or transfer to another facility (e.g., categories '11', '13', '14', '19', '20', '21', '24', '25') will almost exclusively show a 0% readmission rate. This is the most significant **data leakage** observed.\n",
    "*   **`age`**: The `[70-80)` and `[80-90)` age groups appear to have a slightly higher proportion of readmissions compared to younger groups, suggesting it's a predictive feature.\n",
    "*   **`diag_1`, `diag_2`, `diag_3`**: Some specific diagnostic codes might show very high or very low readmission rates. If certain diagnoses are closely tied to the discharge outcome that prevents readmission, they could also reflect leakage indirectly through `discharge_disposition_id`. Without detailed medical knowledge, it's hard to distinguish true causality from leakage for these.\n",
    "*   **`payer_code`**: Certain payer codes might correlate with different healthcare access or follow-up care, leading to varying readmission rates, but no extremely stark differences indicating direct leakage.\n",
    "\n",
    "**Key Takeaway for Leakage:**\n",
    "The feature **`discharge_disposition_id`** is the clearest and most significant indicator of **data leakage**. The information contained in this feature directly determines whether a patient can be readmitted (e.g., if they expired or were transferred to another inpatient facility, they cannot be readmitted to the same hospital within 30 days). Using this feature directly in a predictive model without careful handling would lead to an artificially inflated performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {
    "id": "13"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "# TODO: Add any new imports for your own method here\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "method = 4\n",
    "\n",
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols),\n",
    "    ],\n",
    ")\n",
    "\n",
    "if method==1:\n",
    "    # Use logistic regression model\n",
    "    clf = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=200)),\n",
    "    ])\n",
    "\n",
    "if method==2:\n",
    "    # Use logistic regression model\n",
    "    clf = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=200,class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "if method==3:\n",
    "    # Use SVC (i.e. SVM model)\n",
    "    clf = Pipeline(\n",
    "        [\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"scaler\", StandardScaler(with_mean=False)), # Add StandardScaler here\n",
    "            (\"model\", SVC(gamma=\"auto\",max_iter=1000,probability=True)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "if method == 4:\n",
    "    # Preprocess for HGB: ordinal-encode categories (HGB needs numeric inputs)\n",
    "    preprocess_hgb = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            ]), num_cols),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "            ]), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    clf = Pipeline([\n",
    "        (\"preprocess\", preprocess_hgb),\n",
    "        (\"model\", HistGradientBoostingClassifier(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            max_iter=300,\n",
    "            l2_regularization=1.0,\n",
    "            early_stopping=True,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {
    "id": "14"
   },
   "outputs": [],
   "source": [
    "p_test = clf.predict_proba(test)[:, 1]\n",
    "pred = pd.DataFrame({\"row_id\": test[\"row_id\"].astype(int), \"prob_readmit30\": p_test.astype(float)})\n",
    "pred.to_csv(OUT_PATH, index=False)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {
    "id": "15"
   },
   "outputs": [],
   "source": [
    "# Validate output format (required for students before tagging)\n",
    "!python Project-1/readmit30/scripts/validate_submission.py --pred {OUT_PATH} --test {TEST_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {
    "id": "16"
   },
   "outputs": [],
   "source": [
    "# Calculate metrics for the dev set\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dev = pd.read_csv(DEV_PATH)\n",
    "\n",
    "X_dev = dev.drop(columns=[\"readmit30\"])\n",
    "y_dev = dev[\"readmit30\"].astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = y_dev.astype(int)\n",
    "y_pred = clf.predict_proba(X_dev)[:, 1]\n",
    "\n",
    "auroc = roc_auc_score(y_true, y_pred)\n",
    "auprc = average_precision_score(y_true, y_pred)\n",
    "brier = brier_score_loss(y_true, y_pred)\n",
    "\n",
    "print(f'AUROC: {auroc:.4f}')\n",
    "print(f'AUPRC: {auprc:.4f}')\n",
    "print(f'Brier Score: {brier:.4f}')\n",
    "\n",
    "# Create figures\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogram of predicted probabilities\n",
    "plt.hist(y_pred, bins=20, alpha=0.7, label='Predicted Probabilities')\n",
    "plt.title('Histogram of Predicted Probabilities')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of true vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.5, label='True vs Predicted')\n",
    "plt.title('True vs Predicted Probabilities')\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predicted Probabilities')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUROC = {auroc:.4f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create Precision-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'AUPRC = {auprc:.4f}')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create Confusion Matrix Heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "threshold = 0.5  # Default threshold for binary classification\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Readmit', 'Readmit'], yticklabels=['No Readmit', 'Readmit'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {
    "id": "17"
   },
   "source": [
    "#MAINEND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {
    "id": "wBf42PDniob0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {
    "id": "f74b6093"
   },
   "outputs": [],
   "source": [
    "print(\"Missing values in 'readmit30' column (training data):\", train['readmit30'].isnull().sum())\n",
    "print(\"Missing values in 'readmit30' column (development data):\", dev['readmit30'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {
    "id": "18"
   },
   "source": [
    "## 5) Validate the predictions file format\n",
    "\n",
    "This checks:\n",
    "- required columns\n",
    "- probabilities in [0, 1]\n",
    "- row_ids match the test file\n",
    "\n",
    "It assumes the submission notebook wrote `predictions.csv` in the repo root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {
    "id": "19"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "pred_path = Path(\"predictions.csv\")\n",
    "test_path = Path(\"Project-1/readmit30/scripts/data/public/public_test.csv\")\n",
    "\n",
    "if not pred_path.exists():\n",
    "    print(\"predictions.csv not found. Run notebooks/submission.ipynb first.\")\n",
    "else:\n",
    "    !python Project-1/readmit30/scripts/validate_submission.py --pred predictions.csv --test Project-1/readmit30/scripts/data/public/public_test.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {
    "id": "20"
   },
   "source": [
    "## 6) Commit + push + tag\n",
    "\n",
    "You will:\n",
    "- add changes\n",
    "- commit (pre-commit hook runs here)\n",
    "- push\n",
    "- tag a milestone (example: `milestone_wk3`) and push tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {
    "id": "21"
   },
   "source": [
    "You will need a Personal Access Token (PAT) for the following step. See instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {
    "id": "5mhiRjvTyCQ8"
   },
   "outputs": [],
   "source": [
    "# ==== Colab -> GitHub commit/push for a specific notebook path (PAT auth) ====\n",
    "# What this does:\n",
    "#  1) clones the repo into the Colab VM\n",
    "#  2) overwrites the target notebook file with the *currently open* Colab notebook\n",
    "#  3) commits the change\n",
    "#  4) asks you for a GitHub PAT and pushes to the target branch\n",
    "#  5) (optional) creates a git tag and pushes the tag\n",
    "#\n",
    "# Notes:\n",
    "#  - PAT is read via getpass (not echoed). It is only used for this runtime session.\n",
    "#  - This overwrites the file at TARGET_REL with the *current Colab notebook contents*.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import getpass\n",
    "from google.colab import _message\n",
    "\n",
    "# ==========================\n",
    "# START USER-EDITABLE SETTINGS\n",
    "# ==========================\n",
    "# Repo settings\n",
    "REPO_HTTPS = \"https://github.com/joezein71/AIHC-5010-Winter-2026.git\"  # full https clone URL ending in .git\n",
    "REPO_DIR   = \"AIHC-5010-Winter-2026\"                                # folder name to clone into (or reuse)\n",
    "\n",
    "# Git settings\n",
    "BRANCH     = \"main\"                                                 # branch to commit/push to\n",
    "COMMIT_MSG = \"Update Assignment1_Colab_Workflow.ipynb from Colab test5\"    # commit message\n",
    "\n",
    "# File to overwrite inside the repo (relative to repo root)\n",
    "TARGET_REL = \"Project-1/readmit30/notebooks/Assignment1_Colab_Workflow.ipynb\"\n",
    "\n",
    "# Identity for commits\n",
    "GIT_USER_NAME  = \"Joe Zein\"\n",
    "GIT_USER_EMAIL = \"zein.timothy@mayo.edu\"\n",
    "\n",
    "# (Optional) If you want to push to a different remote than REPO_HTTPS, set it here.\n",
    "# Leave as None to use REPO_HTTPS.\n",
    "PUSH_REMOTE_HTTPS = None  # e.g. \"https://github.com/<user>/<repo>.git\"\n",
    "\n",
    "# Set TAG_NAME to something like \"assignment1-submission-v1\".\n",
    "# Leave as \"\" (empty string) to skip tagging.\n",
    "TAG_NAME    = \"assignment1-submission-v01\"  # e.g. \"assignment1-submission-v1\"\n",
    "TAG_MESSAGE = \"Assignment 1 submission\"  # used only for annotated tags\n",
    "TAG_ANNOTATED = True  # True = annotated tag (-a -m). False = lightweight tag.\n",
    "# ==========================\n",
    "# END USER-EDITABLE SETTINGS\n",
    "# ==========================\n",
    "\n",
    "\n",
    "def run(cmd, cwd=None, check=True):\n",
    "    \"\"\"Run a shell command and stream output.\"\"\"\n",
    "    print(f\"\\n$ {' '.join(cmd)}\")\n",
    "    p = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if p.stdout:\n",
    "        print(p.stdout)\n",
    "    if p.stderr:\n",
    "        print(p.stderr)\n",
    "    if check and p.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {p.returncode}: {' '.join(cmd)}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def github_authed_remote(https_remote: str, token: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert https://github.com/OWNER/REPO.git into https://TOKEN@github.com/OWNER/REPO.git\n",
    "    Works for standard GitHub HTTPS remotes.\n",
    "    \"\"\"\n",
    "    if https_remote.startswith(\"https://\"):\n",
    "        return \"https://\" + token + \"@\" + https_remote[len(\"https://\"):]\n",
    "    raise ValueError(\"Expected an https remote URL (starting with https://).\")\n",
    "\n",
    "\n",
    "def tag_exists_locally(tag_name: str, cwd: str) -> bool:\n",
    "    p = subprocess.run([\"git\", \"tag\", \"-l\", tag_name], cwd=cwd, text=True, capture_output=True)\n",
    "    return p.stdout.strip() == tag_name\n",
    "\n",
    "\n",
    "REMOTE_FOR_PUSH = PUSH_REMOTE_HTTPS or REPO_HTTPS\n",
    "\n",
    "# 1) Clone (or reuse existing clone)\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    run([\"git\", \"clone\", REPO_HTTPS, REPO_DIR])\n",
    "else:\n",
    "    print(f\"Repo directory already exists: {REPO_DIR}\")\n",
    "\n",
    "# Ensure we're on the right branch and up-to-date\n",
    "run([\"git\", \"checkout\", BRANCH], cwd=REPO_DIR)\n",
    "run([\"git\", \"pull\", \"origin\", BRANCH], cwd=REPO_DIR)\n",
    "\n",
    "# 2) Get the currently-open notebook JSON from Colab\n",
    "nb = _message.blocking_request(\"get_ipynb\", timeout_sec=30)[\"ipynb\"]\n",
    "\n",
    "# 3) Overwrite the target file in the clone\n",
    "target_abs = os.path.join(os.getcwd(), REPO_DIR, TARGET_REL)\n",
    "os.makedirs(os.path.dirname(target_abs), exist_ok=True)\n",
    "with open(target_abs, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(nb, f, ensure_ascii=False, indent=1)\n",
    "\n",
    "print(\"Wrote current Colab notebook to:\")\n",
    "print(\" \", target_abs)\n",
    "\n",
    "# 4) Configure git identity\n",
    "run([\"git\", \"config\", \"user.name\", GIT_USER_NAME], cwd=REPO_DIR)\n",
    "run([\"git\", \"config\", \"user.email\", GIT_USER_EMAIL], cwd=REPO_DIR)\n",
    "\n",
    "# 5) Show status; if no changes, stop early\n",
    "status = run([\"git\", \"status\", \"--porcelain\"], cwd=REPO_DIR, check=True).stdout.strip()\n",
    "if not status:\n",
    "    print(\"\\nNo changes detected in the repo after writing the notebook.\")\n",
    "    print(\"Double-check that you're running this cell inside the notebook you edited,\")\n",
    "    print(\"and that TARGET_REL points to the correct path inside the repo.\")\n",
    "else:\n",
    "    # 6) Add + commit\n",
    "    run([\"git\", \"add\", TARGET_REL], cwd=REPO_DIR)\n",
    "\n",
    "    commit_proc = subprocess.run(\n",
    "        [\"git\", \"commit\", \"-m\", COMMIT_MSG],\n",
    "        cwd=REPO_DIR, text=True, capture_output=True\n",
    "    )\n",
    "    if commit_proc.stdout:\n",
    "        print(commit_proc.stdout)\n",
    "    if commit_proc.stderr:\n",
    "        print(commit_proc.stderr)\n",
    "\n",
    "    combined = (commit_proc.stdout + commit_proc.stderr).lower()\n",
    "    if commit_proc.returncode != 0 and \"nothing to commit\" not in combined:\n",
    "        raise RuntimeError(\"git commit failed unexpectedly\")\n",
    "\n",
    "    # 7) Ask for PAT and push\n",
    "    print(\"\\nEnter a GitHub Personal Access Token (PAT) with permission to push to this repo.\")\n",
    "    print(\"Recommended: fine-grained token with access to the repo and Contents: Read/Write.\")\n",
    "    token = getpass.getpass(\"GitHub PAT (input hidden): \").strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"No token entered.\")\n",
    "\n",
    "    # Temporarily set authenticated remote URL for this push only (and for tag push)\n",
    "    authed_remote = github_authed_remote(REMOTE_FOR_PUSH, token)\n",
    "    run([\"git\", \"remote\", \"set-url\", \"origin\", authed_remote], cwd=REPO_DIR)\n",
    "\n",
    "    try:\n",
    "        # Push commits\n",
    "        run([\"git\", \"push\", \"origin\", BRANCH], cwd=REPO_DIR)\n",
    "        print(f\"\\n Pushed successfully to {BRANCH}.\")\n",
    "\n",
    "        # 8) OPTIONAL: Create + push tag\n",
    "        if TAG_NAME.strip():\n",
    "            tag_name = TAG_NAME.strip()\n",
    "\n",
    "            # If tag already exists locally, don't recreate\n",
    "            if tag_exists_locally(tag_name, REPO_DIR):\n",
    "                print(f\"Tag already exists locally: {tag_name}\")\n",
    "            else:\n",
    "                if TAG_ANNOTATED:\n",
    "                    run([\"git\", \"tag\", \"-a\", tag_name, \"-m\", TAG_MESSAGE], cwd=REPO_DIR)\n",
    "                else:\n",
    "                    run([\"git\", \"tag\", tag_name], cwd=REPO_DIR)\n",
    "                print(f\"Created tag: {tag_name}\")\n",
    "\n",
    "            # Push just this tag (or use --tags to push all tags)\n",
    "            run([\"git\", \"push\", \"origin\", tag_name], cwd=REPO_DIR)\n",
    "            print(f\" Pushed tag: {tag_name}\")\n",
    "        else:\n",
    "            print(\"Skipping tag creation (TAG_NAME is empty).\")\n",
    "\n",
    "        print(\"\\nDone. Check GitHub for the new commit (and tag, if set).\")\n",
    "\n",
    "    finally:\n",
    "        # Restore remote URL without token\n",
    "        run([\"git\", \"remote\", \"set-url\", \"origin\", REPO_HTTPS], cwd=REPO_DIR, check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {
    "id": "23"
   },
   "source": [
    "## Done ✅\n",
    "\n",
    "If you hit issues:\n",
    "- Make sure you pulled the latest course template (missing files).\n",
    "- Make sure `data/public/*` exists in your repo (or your instructor provided it separately).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
