{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "id": "0"
   },
   "source": [
    "# Assignment 1 — Colab Workflow (GitHub + Pre-commit + Submission Validation)\n",
    "\n",
    "This notebook teaches the standard workflow used throughout the course:\n",
    "\n",
    "1. Clone your team repo\n",
    "2. Install dependencies\n",
    "3. Install **pre-commit** and enable a hook to strip notebook outputs\n",
    "4. Run this notebook end-to-end\n",
    "5. Validate `predictions.csv`\n",
    "6. Commit + push + tag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1",
    "outputId": "b6adeb1d-aebe-4aae-c943-2b494463d752"
   },
   "outputs": [],
   "source": [
    "# (Colab) show python and system info\n",
    "import sys, platform\n",
    "print(sys.version)\n",
    "print(platform.platform())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {
    "id": "2"
   },
   "source": [
    "## 1) Clone Repo\n",
    "\n",
    "Login to your personal Github account, and make a fork of: https://github.com/TLKline/AIHC-5010-Winter-2026\n",
    "\n",
    "Follow setup directions for working with a PAT in GitHub (30-second guide):\n",
    "\n",
    "* Go to GitHub → Settings\n",
    "* Developer settings\n",
    "* Personal access tokens\n",
    "* Choose:\n",
    "  * Fine-Grained\n",
    "\n",
    "You can clone using HTTPS.\n",
    "\n",
    "Repo HTTPS URL (e.g., `https://github.com/TLKline/AIHC-5010-Winter-2026.git`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3",
    "outputId": "797d9d73-05ee-4708-bd1a-f201604d0100"
   },
   "outputs": [],
   "source": [
    "# TODO: Change the following to your github repo path\n",
    "repo_path = 'https://github.com/TLKline/AIHC-5010-Winter-2026.git'\n",
    "!git clone {repo_path} student_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "id": "HBNUxw47wp-j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4",
    "outputId": "b0b8e03a-3f95-446b-c4fd-805bdd754cb1"
   },
   "outputs": [],
   "source": [
    "# Move into repo\n",
    "%cd student_repo\n",
    "\n",
    "# Repo git info\n",
    "!git status\n",
    "\n",
    "# Where are we?\n",
    "print('----------')\n",
    "print('We are at:')\n",
    "!pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {
    "id": "5"
   },
   "source": [
    "## 2) Install dependencies\n",
    "\n",
    "This installs whatever is in `requirements.txt`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6",
    "outputId": "8719752c-f644-41b8-834a-8ca8835626d8"
   },
   "outputs": [],
   "source": [
    "!pip -q install -r Project-1/readmit30/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "id": "7"
   },
   "source": [
    "## 3) Enable pre-commit hook to strip notebook outputs\n",
    "\n",
    "This prevents giant notebooks and reduces merge/diff pain.\n",
    "\n",
    "One-time per clone:\n",
    "- `pre-commit install`\n",
    "\n",
    "After that, every `git commit` will strip outputs from `*.ipynb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8",
    "outputId": "98fe5638-72c8-43b7-a056-18bf7f4dffe0"
   },
   "outputs": [],
   "source": [
    "!pip -q install pre-commit\n",
    "!pre-commit install\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {
    "id": "9"
   },
   "source": [
    "#MAINSTART"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {
    "id": "10"
   },
   "source": [
    "# 4) Submission Notebook (Template)\n",
    "\n",
    "Replace the baseline model with your team’s approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11",
    "outputId": "599a9e88-cf37-4fe7-c363-7bbbed5fbd62"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "TRAIN_PATH = os.environ.get(\"TRAIN_PATH\", \"Project-1/readmit30/scripts/data/public/train.csv\")\n",
    "DEV_PATH   = os.environ.get(\"DEV_PATH\",   \"Project-1/readmit30/scripts/data/public/dev.csv\")\n",
    "TEST_PATH  = os.environ.get(\"TEST_PATH\",  \"Project-1/readmit30/scripts/data/public/public_test.csv\")\n",
    "OUT_PATH   = os.environ.get(\"OUT_PATH\",   \"predictions.csv\")\n",
    "\n",
    "print(\"TRAIN_PATH:\", TRAIN_PATH)\n",
    "print(\"DEV_PATH:\", DEV_PATH)\n",
    "print(\"TEST_PATH:\", TEST_PATH)\n",
    "print(\"OUT_PATH:\", OUT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {
    "id": "12"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(42)\n",
    "\n",
    "train = pd.read_csv(TRAIN_PATH)\n",
    "test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "assert \"row_id\" in train.columns and \"readmit30\" in train.columns\n",
    "assert \"row_id\" in test.columns\n",
    "\n",
    "X_train = train.drop(columns=[\"readmit30\"])\n",
    "y_train = train[\"readmit30\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "cMTbmKqwTPBv",
    "outputId": "90bf0d7e-b94d-4057-b5cc-f6f305065d54"
   },
   "outputs": [],
   "source": [
    "# EDA - 1 Basic Dataset Snapshot\n",
    "\n",
    "# Rows x columns\n",
    "n_rows, n_cols = train.shape\n",
    "\n",
    "# Outcome column name and overall readmission rate\n",
    "outcome_col = \"readmit30\"\n",
    "readmit_rate = train[outcome_col].mean()\n",
    "\n",
    "# Data types summary\n",
    "cat_cols = [c for c in train.columns if train[c].dtype == \"object\"]\n",
    "num_cols = [c for c in train.columns if c not in cat_cols]\n",
    "\n",
    "# Printed output\n",
    "print(\"Rows × Columns:\", n_rows, \"×\", n_cols)\n",
    "print(f\"Outcome column name: {outcome_col}\")\n",
    "print(f\"Overall readmission rate (mean of readmit30): {readmit_rate:.4f}\")\n",
    "print()\n",
    "print(\"Number of numeric columns:\", len(num_cols))\n",
    "print(\"Number of categorical columns:\", len(cat_cols))\n",
    "print()\n",
    "print(\"Preview of data (df.head()):\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WoeaDTEfUKGt",
    "outputId": "5e20ef34-f9cc-4f45-cd18-8157e2534b0b"
   },
   "outputs": [],
   "source": [
    "# EDA - 2 Missingness Audit\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# A table of missigness per coulmn including count missing and % missing, sorted high to low\n",
    "# Change ? to missing value\n",
    "train = train.replace(\"?\", np.nan)\n",
    "\n",
    "# Total rows for percent calculation\n",
    "n = len(train)\n",
    "\n",
    "# Compute count and percent missing per column\n",
    "missing_df = (\n",
    "    train.isna()\n",
    "         .sum()                               # count missing per column\n",
    "         .to_frame(name=\"missing_count\")\n",
    "         .assign(missing_pct=lambda df: df[\"missing_count\"] / n * 100)\n",
    "         .sort_values([\"missing_count\", \"missing_pct\"], ascending=False)\n",
    ")\n",
    "\n",
    "# A bar plot of the top 15 columns by % missing\n",
    "top15 = missing_df.head(15).iloc[::-1]\n",
    "\n",
    "# Prepare plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars = plt.barh(top15.index, top15[\"missing_pct\"], color=\"#1f77b4\")\n",
    "plt.xlabel(\"Percent missing (%)\")\n",
    "plt.ylabel(\"Column\")\n",
    "plt.title(\"Top 15 columns by percent missing\")\n",
    "for bar, pct in zip(bars, top15[\"missing_pct\"]):\n",
    "    plt.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2,\n",
    "             f\"{pct:.1f}%\", va=\"center\")\n",
    "plt.xlim(0, max(5, top15[\"missing_pct\"].max() * 1.15))\n",
    "plt.tight_layout()\n",
    "\n",
    "# A short list, 3 columns with acceptable missingness (<5%) and 3 columns with problematic missingness (>30%) and recommendation for action (drop vs impute vs investigate collection)\n",
    "acceptable_cols = [\"diabetesMed\", \"insulin\", \"metformin\"]\n",
    "problem_cols = [\"weight\", \"max_glu_serum\", \"A1Cresult\"]\n",
    "action = [\"Drop\", \"Drop\", \"Drop\"]\n",
    "\n",
    "# Printed output - table, plot, 6 bullet point recommendations\n",
    "print(missing_df)\n",
    "plt.show()\n",
    "print(\"Variables With Acceptable Missingness\\n\")\n",
    "for col in acceptable_cols:\n",
    "    missing_count = missing_df.loc[col, \"missing_count\"]\n",
    "    missing_pct = missing_df.loc[col, \"missing_pct\"]\n",
    "    print(f\"\\u2022 Column name: {col}, missing count: {missing_count:,}, missing pct: {missing_pct:.2f}%\")\n",
    "print(\"\\nVariables With Problematic Missingness — Recommended Actions\\n\")\n",
    "for col in problem_cols:\n",
    "    missing_count = missing_df.loc[col, \"missing_count\"]\n",
    "    missing_pct = missing_df.loc[col, \"missing_pct\"]\n",
    "    print(f\"\\u2022 Column name: {col}, missing count: {missing_count:,}, missing pct: {missing_pct:.2f}%, reccomended action: {action[problem_cols.index(col)]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "hKsmu2TCcGvm",
    "outputId": "d24ed22e-a82c-4035-e5c7-e73665107ecc"
   },
   "outputs": [],
   "source": [
    "# EDA - 3 Missingness Related to Outcome?\n",
    "\n",
    "import math\n",
    "\n",
    "# Create a binary indicator\n",
    "for col in problem_cols:\n",
    "    new_col = f\"is_missing_{col}\"\n",
    "    train[new_col] = train[col].replace(\"?\", np.nan).isna().astype(int)\n",
    "\n",
    "# Create a two row table\n",
    "rows = []\n",
    "\n",
    "for col in problem_cols:\n",
    "    indicator = f\"is_missing_{col}\"\n",
    "\n",
    "    # mean(readmit30) gives readmission rate; multiply by 100 for %\n",
    "    rate_not_missing = train.loc[train[indicator] == 0, \"readmit30\"].mean() * 100\n",
    "    rate_missing     = train.loc[train[indicator] == 1, \"readmit30\"].mean() * 100\n",
    "\n",
    "    rows.append({\n",
    "        \"variable\": col,\n",
    "        \"not_missing_rate_pct\": rate_not_missing,\n",
    "        \"missing_rate_pct\": rate_missing\n",
    "    })\n",
    "\n",
    "readmit_missing_table = pd.DataFrame(rows)\n",
    "\n",
    "# Round for display\n",
    "readmit_missing_table = readmit_missing_table.round(2)\n",
    "\n",
    "# Print output\n",
    "readmit_missing_table\n",
    "print(\"Missingness appears (not) associated with outcome; potential MNAR/MAR implications.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "13",
    "outputId": "7e60da2a-549a-4afb-ece5-95d00b5b104c"
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "# TODO: Add any new imports for your own method here\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "method = 4\n",
    "\n",
    "cat_cols = [c for c in X_train.columns if X_train[c].dtype == \"object\"]\n",
    "num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\"))]), num_cols),\n",
    "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), cat_cols),\n",
    "    ],\n",
    ")\n",
    "\n",
    "if method==1:\n",
    "    # Use logistic regression model\n",
    "    clf = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=200)),\n",
    "    ])\n",
    "\n",
    "if method==2:\n",
    "    # Use logistic regression model\n",
    "    clf = Pipeline([\n",
    "        (\"preprocess\", preprocess),\n",
    "        (\"model\", LogisticRegression(max_iter=200,class_weight='balanced')),\n",
    "    ])\n",
    "\n",
    "if method==3:\n",
    "    # Use SVC (i.e. SVM model)\n",
    "    clf = Pipeline(\n",
    "        [\n",
    "            (\"preprocess\", preprocess),\n",
    "            (\"scaler\", StandardScaler(with_mean=False)), # Add StandardScaler here\n",
    "            (\"model\", SVC(gamma=\"auto\",max_iter=1000,probability=True)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "if method == 4:\n",
    "    # Preprocess for HGB: ordinal-encode categories (HGB needs numeric inputs)\n",
    "    preprocess_hgb = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            ]), num_cols),\n",
    "            (\"cat\", Pipeline([\n",
    "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "                (\"ord\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1)),\n",
    "            ]), cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    clf = Pipeline([\n",
    "        (\"preprocess\", preprocess_hgb),\n",
    "        (\"model\", HistGradientBoostingClassifier(\n",
    "            max_depth=6,\n",
    "            learning_rate=0.05,\n",
    "            max_iter=300,\n",
    "            l2_regularization=1.0,\n",
    "            early_stopping=True,\n",
    "            random_state=42,\n",
    "            class_weight='balanced',\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "14",
    "outputId": "3eaa0a3c-c78a-45c4-a396-2d2c389214e2"
   },
   "outputs": [],
   "source": [
    "p_test = clf.predict_proba(test)[:, 1]\n",
    "pred = pd.DataFrame({\"row_id\": test[\"row_id\"].astype(int), \"prob_readmit30\": p_test.astype(float)})\n",
    "pred.to_csv(OUT_PATH, index=False)\n",
    "pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "15",
    "outputId": "34843e33-dae2-4bf3-f6e1-67f43dd63c2b"
   },
   "outputs": [],
   "source": [
    "# Validate output format (required for students before tagging)\n",
    "!python Project-1/readmit30/scripts/validate_submission.py --pred {OUT_PATH} --test {TEST_PATH}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "16",
    "outputId": "8ca8baec-a623-4d7c-fcfa-656cdcae742a"
   },
   "outputs": [],
   "source": [
    "# Calculate metrics for the dev set\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dev = pd.read_csv(DEV_PATH)\n",
    "\n",
    "X_dev = dev.drop(columns=[\"readmit30\"])\n",
    "y_dev = dev[\"readmit30\"].astype(int)\n",
    "\n",
    "# Calculate metrics\n",
    "y_true = y_dev.astype(int)\n",
    "y_pred = clf.predict_proba(X_dev)[:, 1]\n",
    "\n",
    "auroc = roc_auc_score(y_true, y_pred)\n",
    "auprc = average_precision_score(y_true, y_pred)\n",
    "brier = brier_score_loss(y_true, y_pred)\n",
    "\n",
    "print(f'AUROC: {auroc:.4f}')\n",
    "print(f'AUPRC: {auprc:.4f}')\n",
    "print(f'Brier Score: {brier:.4f}')\n",
    "\n",
    "# Create figures\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Histogram of predicted probabilities\n",
    "plt.hist(y_pred, bins=20, alpha=0.7, label='Predicted Probabilities')\n",
    "plt.title('Histogram of Predicted Probabilities')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot of true vs predicted\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_true, y_pred, alpha=0.5, label='True vs Predicted')\n",
    "plt.title('True vs Predicted Probabilities')\n",
    "plt.xlabel('True Labels')\n",
    "plt.ylabel('Predicted Probabilities')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create ROC Curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, label=f'AUROC = {auroc:.4f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title('ROC Curve')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create Precision-Recall Curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, label=f'AUPRC = {auprc:.4f}')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Create Confusion Matrix Heatmap\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "threshold = 0.5  # Default threshold for binary classification\n",
    "y_pred_binary = (y_pred >= threshold).astype(int)\n",
    "cm = confusion_matrix(y_true, y_pred_binary)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Readmit', 'Readmit'], yticklabels=['No Readmit', 'Readmit'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {
    "id": "17"
   },
   "source": [
    "#MAINEND"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {
    "id": "18"
   },
   "source": [
    "## 5) Validate the predictions file format\n",
    "\n",
    "This checks:\n",
    "- required columns\n",
    "- probabilities in [0, 1]\n",
    "- row_ids match the test file\n",
    "\n",
    "It assumes the submission notebook wrote `predictions.csv` in the repo root.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19",
    "outputId": "bfc745ef-741c-4f11-b3f1-074587b14422"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "pred_path = Path(\"predictions.csv\")\n",
    "test_path = Path(\"Project-1/readmit30/scripts/data/public/public_test.csv\")\n",
    "\n",
    "if not pred_path.exists():\n",
    "    print(\"predictions.csv not found. Run notebooks/submission.ipynb first.\")\n",
    "else:\n",
    "    !python Project-1/readmit30/scripts/validate_submission.py --pred predictions.csv --test Project-1/readmit30/scripts/data/public/public_test.csv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {
    "id": "20"
   },
   "source": [
    "## 6) Commit + push + tag\n",
    "\n",
    "You will:\n",
    "- add changes\n",
    "- commit (pre-commit hook runs here)\n",
    "- push\n",
    "- tag a milestone (example: `milestone_wk3`) and push tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {
    "id": "21"
   },
   "source": [
    "You will need a Personal Access Token (PAT) for the following step. See instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mhiRjvTyCQ8",
    "outputId": "9eb746df-abd3-4f02-97fd-034bc78296b8"
   },
   "outputs": [],
   "source": [
    "# ==== Colab -> GitHub commit/push for a specific notebook path (PAT auth) ====\n",
    "# What this does:\n",
    "#  1) clones the repo into the Colab VM\n",
    "#  2) overwrites the target notebook file with the *currently open* Colab notebook\n",
    "#  3) commits the change\n",
    "#  4) asks you for a GitHub PAT and pushes to the target branch\n",
    "#  5) (optional) creates a git tag and pushes the tag\n",
    "#\n",
    "# Notes:\n",
    "#  - PAT is read via getpass (not echoed). It is only used for this runtime session.\n",
    "#  - This overwrites the file at TARGET_REL with the *current Colab notebook contents*.\n",
    "\n",
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "import getpass\n",
    "from google.colab import _message\n",
    "\n",
    "# ==========================\n",
    "# START USER-EDITABLE SETTINGS\n",
    "# ==========================\n",
    "# Repo settings\n",
    "REPO_HTTPS = \"https://github.com/tylerlherzog/AIHC-5010-Winter-2026.git\"  # full https clone URL ending in .git\n",
    "REPO_DIR   = \"AIHC-5010-Winter-2026\"                                # folder name to clone into (or reuse)\n",
    "\n",
    "# Git settings\n",
    "BRANCH     = \"main\"                                                 # branch to commit/push to\n",
    "COMMIT_MSG = \"Update Assignment1_Colab_Workflow.ipynb from Colab test5\"    # commit message\n",
    "\n",
    "# File to overwrite inside the repo (relative to repo root)\n",
    "TARGET_REL = \"Project-1/readmit30/notebooks/Assignment1_Colab_Workflow.ipynb\"\n",
    "\n",
    "# Identity for commits\n",
    "GIT_USER_NAME  = \"Tyler Herzog\"\n",
    "GIT_USER_EMAIL = \"tylerlherzog@gmail.com\"\n",
    "\n",
    "# (Optional) If you want to push to a different remote than REPO_HTTPS, set it here.\n",
    "# Leave as None to use REPO_HTTPS.\n",
    "PUSH_REMOTE_HTTPS = None  # e.g. \"https://github.com/<user>/<repo>.git\"\n",
    "\n",
    "# Set TAG_NAME to something like \"assignment1-submission-v1\".\n",
    "# Leave as \"\" (empty string) to skip tagging.\n",
    "TAG_NAME    = \"\"  # e.g. \"assignment1-submission-v1\"\n",
    "TAG_MESSAGE = \"Assignment 1 submission\"  # used only for annotated tags\n",
    "TAG_ANNOTATED = True  # True = annotated tag (-a -m). False = lightweight tag.\n",
    "# ==========================\n",
    "# END USER-EDITABLE SETTINGS\n",
    "# ==========================\n",
    "\n",
    "\n",
    "def run(cmd, cwd=None, check=True):\n",
    "    \"\"\"Run a shell command and stream output.\"\"\"\n",
    "    print(f\"\\n$ {' '.join(cmd)}\")\n",
    "    p = subprocess.run(cmd, cwd=cwd, text=True, capture_output=True)\n",
    "    if p.stdout:\n",
    "        print(p.stdout)\n",
    "    if p.stderr:\n",
    "        print(p.stderr)\n",
    "    if check and p.returncode != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {p.returncode}: {' '.join(cmd)}\")\n",
    "    return p\n",
    "\n",
    "\n",
    "def github_authed_remote(https_remote: str, token: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert https://github.com/OWNER/REPO.git into https://TOKEN@github.com/OWNER/REPO.git\n",
    "    Works for standard GitHub HTTPS remotes.\n",
    "    \"\"\"\n",
    "    if https_remote.startswith(\"https://\"):\n",
    "        return \"https://\" + token + \"@\" + https_remote[len(\"https://\"):]\n",
    "    raise ValueError(\"Expected an https remote URL (starting with https://).\")\n",
    "\n",
    "\n",
    "def tag_exists_locally(tag_name: str, cwd: str) -> bool:\n",
    "    p = subprocess.run([\"git\", \"tag\", \"-l\", tag_name], cwd=cwd, text=True, capture_output=True)\n",
    "    return p.stdout.strip() == tag_name\n",
    "\n",
    "\n",
    "REMOTE_FOR_PUSH = PUSH_REMOTE_HTTPS or REPO_HTTPS\n",
    "\n",
    "# 1) Clone (or reuse existing clone)\n",
    "if not os.path.isdir(REPO_DIR):\n",
    "    run([\"git\", \"clone\", REPO_HTTPS, REPO_DIR])\n",
    "else:\n",
    "    print(f\"Repo directory already exists: {REPO_DIR}\")\n",
    "\n",
    "# Ensure we're on the right branch and up-to-date\n",
    "run([\"git\", \"checkout\", BRANCH], cwd=REPO_DIR)\n",
    "run([\"git\", \"pull\", \"origin\", BRANCH], cwd=REPO_DIR)\n",
    "\n",
    "# 2) Get the currently-open notebook JSON from Colab\n",
    "nb = _message.blocking_request(\"get_ipynb\", timeout_sec=30)[\"ipynb\"]\n",
    "\n",
    "# 3) Overwrite the target file in the clone\n",
    "target_abs = os.path.join(os.getcwd(), REPO_DIR, TARGET_REL)\n",
    "os.makedirs(os.path.dirname(target_abs), exist_ok=True)\n",
    "with open(target_abs, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(nb, f, ensure_ascii=False, indent=1)\n",
    "\n",
    "print(\"Wrote current Colab notebook to:\")\n",
    "print(\" \", target_abs)\n",
    "\n",
    "# 4) Configure git identity\n",
    "run([\"git\", \"config\", \"user.name\", GIT_USER_NAME], cwd=REPO_DIR)\n",
    "run([\"git\", \"config\", \"user.email\", GIT_USER_EMAIL], cwd=REPO_DIR)\n",
    "\n",
    "# 5) Show status; if no changes, stop early\n",
    "status = run([\"git\", \"status\", \"--porcelain\"], cwd=REPO_DIR, check=True).stdout.strip()\n",
    "if not status:\n",
    "    print(\"\\nNo changes detected in the repo after writing the notebook.\")\n",
    "    print(\"Double-check that you're running this cell inside the notebook you edited,\")\n",
    "    print(\"and that TARGET_REL points to the correct path inside the repo.\")\n",
    "else:\n",
    "    # 6) Add + commit\n",
    "    run([\"git\", \"add\", TARGET_REL], cwd=REPO_DIR)\n",
    "\n",
    "    commit_proc = subprocess.run(\n",
    "        [\"git\", \"commit\", \"-m\", COMMIT_MSG],\n",
    "        cwd=REPO_DIR, text=True, capture_output=True\n",
    "    )\n",
    "    if commit_proc.stdout:\n",
    "        print(commit_proc.stdout)\n",
    "    if commit_proc.stderr:\n",
    "        print(commit_proc.stderr)\n",
    "\n",
    "    combined = (commit_proc.stdout + commit_proc.stderr).lower()\n",
    "    if commit_proc.returncode != 0 and \"nothing to commit\" not in combined:\n",
    "        raise RuntimeError(\"git commit failed unexpectedly\")\n",
    "\n",
    "    # 7) Ask for PAT and push\n",
    "    print(\"\\nEnter a GitHub Personal Access Token (PAT) with permission to push to this repo.\")\n",
    "    print(\"Recommended: fine-grained token with access to the repo and Contents: Read/Write.\")\n",
    "    token = getpass.getpass(\"GitHub PAT (input hidden): \").strip()\n",
    "    if not token:\n",
    "        raise ValueError(\"No token entered.\")\n",
    "\n",
    "    # Temporarily set authenticated remote URL for this push only (and for tag push)\n",
    "    authed_remote = github_authed_remote(REMOTE_FOR_PUSH, token)\n",
    "    run([\"git\", \"remote\", \"set-url\", \"origin\", authed_remote], cwd=REPO_DIR)\n",
    "\n",
    "    try:\n",
    "        # Push commits\n",
    "        run([\"git\", \"push\", \"origin\", BRANCH], cwd=REPO_DIR)\n",
    "        print(f\"\\n Pushed successfully to {BRANCH}.\")\n",
    "\n",
    "        # 8) OPTIONAL: Create + push tag\n",
    "        if TAG_NAME.strip():\n",
    "            tag_name = TAG_NAME.strip()\n",
    "\n",
    "            # If tag already exists locally, don't recreate\n",
    "            if tag_exists_locally(tag_name, REPO_DIR):\n",
    "                print(f\"Tag already exists locally: {tag_name}\")\n",
    "            else:\n",
    "                if TAG_ANNOTATED:\n",
    "                    run([\"git\", \"tag\", \"-a\", tag_name, \"-m\", TAG_MESSAGE], cwd=REPO_DIR)\n",
    "                else:\n",
    "                    run([\"git\", \"tag\", tag_name], cwd=REPO_DIR)\n",
    "                print(f\"Created tag: {tag_name}\")\n",
    "\n",
    "            # Push just this tag (or use --tags to push all tags)\n",
    "            run([\"git\", \"push\", \"origin\", tag_name], cwd=REPO_DIR)\n",
    "            print(f\" Pushed tag: {tag_name}\")\n",
    "        else:\n",
    "            print(\"Skipping tag creation (TAG_NAME is empty).\")\n",
    "\n",
    "        print(\"\\nDone. Check GitHub for the new commit (and tag, if set).\")\n",
    "\n",
    "    finally:\n",
    "        # Restore remote URL without token\n",
    "        run([\"git\", \"remote\", \"set-url\", \"origin\", REPO_HTTPS], cwd=REPO_DIR, check=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {
    "id": "23"
   },
   "source": [
    "## Done ✅\n",
    "\n",
    "If you hit issues:\n",
    "- Make sure you pulled the latest course template (missing files).\n",
    "- Make sure `data/public/*` exists in your repo (or your instructor provided it separately).\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
