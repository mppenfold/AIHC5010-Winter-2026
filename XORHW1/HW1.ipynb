{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW1 Perceptron and XOR\n",
    "\n",
    "\n",
    "Implement a perceptron and train the model on an example dataset.\n",
    "\n",
    "Dataset 1: a linearly separable dataset.\n",
    "\n",
    "Dataset 2: a model of the XOR problem.\n",
    "\n",
    "Extra credit: create a simple model that can be trained to solve XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perceptron implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__(self, learning_rate=0.01, n_iters=1000):\n",
    "        self.lr = learning_rate\n",
    "        self.n_iters = n_iters\n",
    "        self.activation_func = self._unit_step_func\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def _unit_step_func(self, x):\n",
    "        return np.where(x >= 0, 1, 0)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize parameters\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        y_ = np.array([1 if i > 0 else 0 for i in y])\n",
    "\n",
    "        for _ in range(self.n_iters):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                linear_output = np.dot(x_i, self.weights) + self.bias\n",
    "                y_predicted = self.activation_func(linear_output)\n",
    "\n",
    "                # Perceptron update rule\n",
    "                update = self.lr * (y_[idx] - y_predicted)\n",
    "                self.weights += update * x_i\n",
    "                self.bias += update\n",
    "\n",
    "    def predict(self, X):\n",
    "        linear_output = np.dot(X, self.weights) + self.bias\n",
    "        y_predicted = self.activation_func(linear_output)\n",
    "        return y_predicted\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        predictions = self.predict(X)\n",
    "        return np.mean(predictions == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linearly Separable Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate linearly separable data\n",
    "X_linear, y_linear = make_blobs(n_samples=100, centers=2, n_features=2, random_state=42, cluster_std=1.5)\n",
    "\n",
    "# Plot the data\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_linear[:, 0], X_linear[:, 1], c=y_linear, cmap='bwr', edgecolor='k')\n",
    "plt.title('Linearly Separable Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Perceptron\n",
    "p_linear = Perceptron(learning_rate=0.01, n_iters=1000)\n",
    "p_linear.fit(X_linear, y_linear)\n",
    "acc = p_linear.score(X_linear, y_linear)\n",
    "print(f\"Perceptron classification accuracy on linear dataset: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization function for decision boundary\n",
    "def plot_decision_boundary(X, y, classifier, title):\n",
    "    x0_min, x0_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x1_min, x1_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx0, xx1 = np.meshgrid(np.arange(x0_min, x0_max, 0.02),\n",
    "                           np.arange(x1_min, x1_max, 0.02))\n",
    "    \n",
    "    Z = classifier.predict(np.c_[xx0.ravel(), xx1.ravel()])\n",
    "    Z = Z.reshape(xx0.shape)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx0, xx1, Z, alpha=0.3, cmap='bwr')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='bwr', edgecolor='k')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(X_linear, y_linear, p_linear, 'Perceptron on Linearly Separable Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset 2: XOR Problem\n",
    "\n",
    "Attempt at training perceptron on XOR problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR Data\n",
    "X_xor = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "y_xor = np.array([0, 1, 1, 0])\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_xor[:, 0], X_xor[:, 1], c=y_xor, cmap='bwr', s=200, edgecolor='k')\n",
    "plt.title('XOR Dataset')\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Perceptron on XOR\n",
    "p_xor = Perceptron(learning_rate=0.1, n_iters=1000)\n",
    "p_xor.fit(X_xor, y_xor)\n",
    "acc_xor = p_xor.score(X_xor, y_xor)\n",
    "print(f\"Perceptron classification accuracy on XOR dataset: {acc_xor * 100:.2f}%\")\n",
    "\n",
    "plot_decision_boundary(X_xor, y_xor, p_xor, 'Perceptron on XOR Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Single layer perceptron unable to classify XOR because it can only draw a straight line. XOR requires a non-linear decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra Credit: Model trained to solve XOR\n",
    "\n",
    "Non-linear activation function is required to solve XOR. Below with one hidden layer, non-linear activation function (tanh) is used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.1):\n",
    "        # Initialize weights randomly\n",
    "        self.W1 = np.random.randn(input_size, hidden_size)\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size)\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def train(self, X, y, epochs=10000):\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        losses = []\n",
    "        \n",
    "        for _ in range(epochs):\n",
    "            # Forward pass\n",
    "            # Layer 1\n",
    "            z1 = np.dot(X, self.W1) + self.b1\n",
    "            a1 = self.sigmoid(z1)\n",
    "            \n",
    "            # Layer 2 (Output)\n",
    "            z2 = np.dot(a1, self.W2) + self.b2\n",
    "            output = self.sigmoid(z2)\n",
    "            \n",
    "            # Compute Loss (Mean Squared Error)\n",
    "            error = y - output\n",
    "            loss = np.mean(np.square(error))\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backpropagation\n",
    "            # Output layer gradients\n",
    "            d_output = error * self.sigmoid_derivative(output)\n",
    "            \n",
    "            # Hidden layer gradients\n",
    "            error_hidden = d_output.dot(self.W2.T)\n",
    "            d_hidden = error_hidden * self.sigmoid_derivative(a1)\n",
    "            \n",
    "            # Update weights and biases\n",
    "            self.W2 += a1.T.dot(d_output) * self.lr\n",
    "            self.b2 += np.sum(d_output, axis=0, keepdims=True) * self.lr\n",
    "            self.W1 += X.T.dot(d_hidden) * self.lr\n",
    "            self.b1 += np.sum(d_hidden, axis=0, keepdims=True) * self.lr\n",
    "            \n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Forward pass only\n",
    "        z1 = np.dot(X, self.W1) + self.b1\n",
    "        a1 = self.sigmoid(z1)\n",
    "        z2 = np.dot(a1, self.W2) + self.b2\n",
    "        output = self.sigmoid(z2)\n",
    "        return np.where(output > 0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train MLP on XOR\n",
    "# Architecture: 2 inputs -> 4 hidden neurons -> 1 output\n",
    "mlp = SimpleMLP(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n",
    "\n",
    "loss_history = mlp.train(X_xor, y_xor, epochs=10000)\n",
    "\n",
    "# Plot Loss curve\n",
    "plt.plot(loss_history)\n",
    "plt.title('MLP Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Predictions\n",
    "predictions = mlp.predict(X_xor)\n",
    "print(\"MLP Predictions on XOR:\")\n",
    "for i, x in enumerate(X_xor):\n",
    "    print(f\"Input: {x}, Target: {y_xor[i]}, Predicted: {predictions[i][0]}\")\n",
    "\n",
    "plot_decision_boundary(X_xor, y_xor, mlp, 'MLP on XOR Data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
